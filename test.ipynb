{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc113b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "AIzaSyBS-2pbdjYouOkcqHaX4ZI5HHPpSSmq3iw\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91a38983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Section 1 ===\n",
      "\n",
      "Generating TYPE: Table Completion | Questions 1-10 (count=10)\n",
      " Attempt 1/5\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 7 fields in line 31, saw 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 354\u001b[0m\n\u001b[0;32m    351\u001b[0m specific_topic\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniversity Lectures\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    352\u001b[0m specifications\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcademic context, formal tone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 354\u001b[0m \u001b[43mgenerate_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheme\u001b[49m\u001b[43m,\u001b[49m\u001b[43mspecific_topic\u001b[49m\u001b[43m,\u001b[49m\u001b[43mspecifications\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m save_all_outputs(questions)\n",
      "Cell \u001b[1;32mIn[16], line 233\u001b[0m, in \u001b[0;36mgenerate_question\u001b[1;34m(theme, specific_topic, specifications)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_in_average_word_count(transcript, section_label): reward\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m calculate_common_word_ratio(transcript)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m: reward\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcalculate_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscript\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m: reward\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward\u001b[38;5;241m>\u001b[39mbest_reward:\n\u001b[0;32m    236\u001b[0m     best_reward\u001b[38;5;241m=\u001b[39mreward\n",
      "Cell \u001b[1;32mIn[16], line 112\u001b[0m, in \u001b[0;36mcalculate_similarity\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    110\u001b[0m     existing_texts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m training_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(GENERATED_CSV):\n\u001b[1;32m--> 112\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGENERATED_CSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranscript\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m    114\u001b[0m         existing_texts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranscript\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\environment\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\environment\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\environment\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\environment\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 7 fields in line 31, saw 8\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# IELTS Listening Question Generator\n",
    "# -----------------------------\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import json5\n",
    "import random\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import textstat\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "genai.configure(api_key=\"AIzaSyBS-2pbdjYouOkcqHaX4ZI5HHPpSSmq3iw\")  # replace with your key\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "QUESTION_TYPE_CSV = \"model_training/processed_data/questionType.csv\"\n",
    "TRAINING_CSV = \"model_training/processed_data/training_set.csv\"\n",
    "GENERATED_CSV = \"model_training/processed_data/generated_question.csv\"\n",
    "WORD_CSV = \"model_training/processed_data/ielts_vocab.csv\"\n",
    "\n",
    "MAX_ATTEMPT = 5\n",
    "REWARD_GOAL = 4\n",
    "\n",
    "assignments = {}\n",
    "questions = []\n",
    "\n",
    "# Load reference data\n",
    "question_type_df = pd.read_csv(QUESTION_TYPE_CSV)\n",
    "common_vocab_df = pd.read_csv(WORD_CSV)\n",
    "training_df = pd.read_csv(TRAINING_CSV)\n",
    "\n",
    "# -----------------------------\n",
    "# PROMPT TEMPLATE\n",
    "# -----------------------------\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert IELTS Listening question generator.\n",
    "Generate realistic IELTS Listening questions for the requested section and question range.\n",
    "\n",
    "SECTION: {section}\n",
    "Question Numbers: {question_range}\n",
    "Number of questions: {question_count}\n",
    "Question Type: {typeID} - {type_name}\n",
    "Theme: {theme}\n",
    "Specific Topic: {specific_topic}\n",
    "Specifications: {specifications}\n",
    "Instructions: {instruction}\n",
    "Answer Format: {answer_format}\n",
    "Format: {format}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1) You MUST produce exactly {question_count} Questions, Answers, and for multiple choice: Options array in JSON.\n",
    "2) Each Options array must correspond to its question, e.g., Options[0] = [\"A. ...\",\"B. ...\",...].\n",
    "3) The Transcript MUST mention the correct question range, e.g. \"Now you have time to look at questions {question_range}\".\n",
    "4) Return ONLY JSON in this schema:\n",
    "\n",
    "{{\n",
    "  \"Section\": \"{section}\",\n",
    "  \"Type\": \"{type_name}\",\n",
    "  \"Instructions\": \"{instruction}\",\n",
    "  \"Diagram\": \"<optional diagram>\",\n",
    "  \"Questions\": [\"question text 1\", \"...\", \"question text N\"],\n",
    "  \"Options\": [ [\"A. ...\",\"B. ...\"], [...], ... ], \n",
    "  \"Answers\": [\"answer1\",\"answer2\",...],\n",
    "  \"Transcript\": \"<full transcript text>\"\n",
    "}}\n",
    "\n",
    "Do not include any explanatory text outside JSON.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# UTILITY FUNCTIONS\n",
    "# -----------------------------\n",
    "def choose_question_type():\n",
    "    for section in sorted(question_type_df['part'].unique()):\n",
    "        section_types = question_type_df[question_type_df['part'] == section]['type'].tolist()\n",
    "        k = random.choice([1,2])\n",
    "        selected = random.sample(section_types, k=min(k,len(section_types)))\n",
    "        assignments[f\"Section {section}\"] = selected\n",
    "    return assignments\n",
    "\n",
    "def calculate_readability_score(text):\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "def is_in_average_word_count(text, section_label):\n",
    "    def count_words(txt):\n",
    "        if isinstance(txt, (list, pd.Series)):\n",
    "            txt = \" \".join(map(str, txt))\n",
    "        return len(re.findall(r'\\b\\w+\\b', str(txt)))\n",
    "    ranges = {\"Section 1\": (500,700),\"Section 2\":(600,800),\"Section 3\":(800,1000),\"Section 4\":(1000,1200)}\n",
    "    total = count_words(text)\n",
    "    low, high = ranges.get(section_label,(0,1e9))\n",
    "    return low <= total <= high\n",
    "\n",
    "def calculate_common_word_ratio(text):\n",
    "    vocab = set(common_vocab_df[\"Words\"].astype(str).str.lower())\n",
    "    if pd.isna(text) or not str(text).strip(): return 0\n",
    "    words = [w.lower() for w in re.findall(r'\\b\\w+\\b', str(text))]\n",
    "    if not words: return 0\n",
    "    return len([w for w in words if w not in vocab]) / len(words)\n",
    "\n",
    "def calculate_similarity(text):\n",
    "    existing_texts = []\n",
    "    if 'transcript' in training_df.columns:\n",
    "        existing_texts += training_df['transcript'].dropna().astype(str).tolist()\n",
    "    if os.path.exists(GENERATED_CSV):\n",
    "        df = pd.read_csv(GENERATED_CSV)\n",
    "        if 'Transcript' in df.columns:\n",
    "            existing_texts += df['Transcript'].dropna().astype(str).tolist()\n",
    "    if not existing_texts: return 0.0\n",
    "    corpus = existing_texts + [str(text)]\n",
    "    vecs = TfidfVectorizer().fit(corpus).transform(corpus)\n",
    "    sim_scores = cosine_similarity(vecs[-1], vecs[:-1]).flatten()\n",
    "    return sim_scores.max() if len(sim_scores) else 0.0\n",
    "\n",
    "def safe_json_parse(raw_text):\n",
    "    if not raw_text: return None\n",
    "    text = raw_text.replace(\"```json\",\"\").replace(\"```\",\"\").strip()\n",
    "    obj_match = re.search(r'({[\\s\\S]*})', text)\n",
    "    if not obj_match: return None\n",
    "    try:\n",
    "        return json5.loads(obj_match.group(1))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def convert_to_df(model_json, section_label):\n",
    "    if model_json is None: return pd.DataFrame([None])\n",
    "    if isinstance(model_json, dict): records = [model_json]\n",
    "    elif isinstance(model_json, list): records = model_json\n",
    "    else: return pd.DataFrame([None])\n",
    "\n",
    "    normalized = []\n",
    "    for q in records:\n",
    "        if not q: continue\n",
    "        clean = {}\n",
    "        clean['DateTime_Generated'] = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "        clean['Section'] = q.get('Section', section_label)\n",
    "        clean['Type'] = \" \".join(q['Type']) if isinstance(q.get(\"Type\"), list) else q.get(\"Type\")\n",
    "        clean['Instructions'] = \" \".join(q['Instructions']) if isinstance(q.get(\"Instructions\"), list) else q.get(\"Instructions\")\n",
    "        clean['Diagram'] = q.get(\"Diagram\", None)\n",
    "        clean['Questions'] = q.get(\"Questions\")\n",
    "        clean['Options'] = q.get(\"Options\", [None]*len(q.get(\"Questions\",[])))\n",
    "        clean['Answers'] = q.get(\"Answers\")\n",
    "        clean['Transcript'] = q.get(\"Transcript\")\n",
    "        normalized.append(clean)\n",
    "    return pd.DataFrame(normalized)\n",
    "\n",
    "def get_question_counts(types):\n",
    "    if len(types) == 1: return {types[0]:10}\n",
    "    elif len(types) == 2: return {types[0]:5, types[1]:5}\n",
    "    else: return {t: max(1,10//len(types)) for t in types}\n",
    "\n",
    "def question_number_ranges_for_section(counts, section_number):\n",
    "    base = (section_number-1)*10+1\n",
    "    ranges = {}\n",
    "    current = base\n",
    "    for typ, cnt in counts.items():\n",
    "        start = current\n",
    "        end = current+cnt-1\n",
    "        ranges[typ] = f\"{start}-{end}\"\n",
    "        current = end+1\n",
    "    return ranges\n",
    "\n",
    "def generate_question_model(typeID,type_name,section,question_range,question_count,theme,specific_topic,specifications,instruction,answer_format,format,key_skills,avg_duration,avg_script_length,key_features,audio_speed):\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        section=section,\n",
    "        question_range=question_range,\n",
    "        question_count=question_count,\n",
    "        typeID=typeID,\n",
    "        type_name=type_name,\n",
    "        theme=theme,\n",
    "        specific_topic=specific_topic,\n",
    "        specifications=specifications,\n",
    "        instruction=instruction,\n",
    "        answer_format=answer_format,\n",
    "        format=format,\n",
    "        key_skills=key_skills,\n",
    "        avg_duration=avg_duration,\n",
    "        avg_script_length=avg_script_length,\n",
    "        key_features=key_features,\n",
    "        audio_speed=audio_speed\n",
    "    )\n",
    "    response = model.generate_content(prompt)\n",
    "    return safe_json_parse(response.text)\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN GENERATION LOOP\n",
    "# -----------------------------\n",
    "def generate_question(theme, specific_topic, specifications):\n",
    "    assignments = choose_question_type()\n",
    "    for section_label, types in assignments.items():\n",
    "        sec_num = int(re.search(r'(\\d+)', section_label).group(1))\n",
    "        print(f\"\\n=== {section_label} ===\")\n",
    "        counts = get_question_counts(types)\n",
    "        ranges = question_number_ranges_for_section(counts, sec_num)\n",
    "\n",
    "        for q_type in types:\n",
    "            type_info = question_type_df[question_type_df['type']==q_type].iloc[0]\n",
    "            question_count = counts[q_type]\n",
    "            question_range = ranges[q_type]\n",
    "            print(f\"\\nGenerating TYPE: {q_type} | Questions {question_range} (count={question_count})\")\n",
    "            best_reward = -1\n",
    "            best_record = None\n",
    "            for attempt in range(MAX_ATTEMPT):\n",
    "                print(f\" Attempt {attempt+1}/{MAX_ATTEMPT}\")\n",
    "                model_json = generate_question_model(\n",
    "                    typeID=type_info['type'], type_name=type_info['type'], section=section_label,\n",
    "                    question_range=question_range, question_count=question_count,\n",
    "                    theme=theme, specific_topic=specific_topic, specifications=specifications,\n",
    "                    instruction=type_info['instruction'], answer_format=type_info['answer_format'],\n",
    "                    format=type_info['format'], key_skills=type_info['key_skills'],\n",
    "                    avg_duration=type_info['avg_duration'], avg_script_length=type_info['avg_script_length'],\n",
    "                    key_features=type_info['key_features'], audio_speed=type_info['audio_speed']\n",
    "                )\n",
    "\n",
    "                if not isinstance(model_json, dict): continue\n",
    "                q_list = model_json.get(\"Questions\")\n",
    "                a_list = model_json.get(\"Answers\")\n",
    "                if not isinstance(q_list,list) or not isinstance(a_list,list): continue\n",
    "                if len(q_list)!=question_count or len(a_list)!=question_count: continue\n",
    "                transcript = model_json.get(\"Transcript\",\"\")\n",
    "                if str(question_range).split('-')[0] not in transcript: continue\n",
    "\n",
    "                reward = 0\n",
    "                if calculate_readability_score(transcript)>=60: reward+=1\n",
    "                if is_in_average_word_count(transcript, section_label): reward+=1\n",
    "                if calculate_common_word_ratio(transcript)>=0.1: reward+=1\n",
    "                if calculate_similarity(transcript)<=0.85: reward+=1\n",
    "\n",
    "                if reward>best_reward:\n",
    "                    best_reward=reward\n",
    "                    best_record=model_json\n",
    "                if reward==REWARD_GOAL: break\n",
    "\n",
    "            if best_record is None:\n",
    "                placeholder={\"Section\":section_label,\"Type\":type_info['type'],\n",
    "                             \"Instructions\":type_info['instruction'],\"Diagram\":None,\n",
    "                             \"Questions\":[f\"Placeholder Q{i}\" for i in range(1,question_count+1)],\n",
    "                             \"Options\":[[f\"A{i}\",\"B{i}\",\"C{i}\",\"D{i}\"] for i in range(1,question_count+1)],\n",
    "                             \"Answers\":[f\"Answer{i}\" for i in range(1,question_count+1)],\n",
    "                             \"Transcript\":f\"Placeholder transcript mentioning questions {question_range}.\"}\n",
    "                best_record=placeholder\n",
    "\n",
    "            temp_df = convert_to_df(best_record, section_label)\n",
    "            questions.append(temp_df)\n",
    "    print(\"\\n=== Generation COMPLETE ===\")\n",
    "\n",
    "# -----------------------------\n",
    "# SAVE FILES\n",
    "# -----------------------------\n",
    "def make_json_safe(obj):\n",
    "    try: return json.dumps(obj, ensure_ascii=False)\n",
    "    except: return str(obj)\n",
    "\n",
    "def save_csv(df):\n",
    "    os.makedirs(\"model_training/processed_data\", exist_ok=True)\n",
    "    safe_df = df.copy()\n",
    "    for col in [\"Questions\",\"Answers\",\"Transcript\",\"Instructions\",\"Options\"]:\n",
    "        safe_df[col]=safe_df[col].apply(make_json_safe)\n",
    "    if os.path.exists(GENERATED_CSV):\n",
    "        safe_df.to_csv(GENERATED_CSV, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        safe_df.to_csv(GENERATED_CSV,index=False)\n",
    "    print(f\"Saved generated questions to {GENERATED_CSV}\")\n",
    "\n",
    "def create_set_folder():\n",
    "    os.makedirs(\"set\", exist_ok=True)\n",
    "    existing = [d for d in os.listdir(\"set\") if d.startswith(\"set\")]\n",
    "    next_id = len(existing)+1\n",
    "    folder = f\"set/set{next_id}\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    return folder\n",
    "\n",
    "def write_text_files(folder, df):\n",
    "    df_sorted = df.sort_values(by=\"Section\")\n",
    "    question_counter = 1\n",
    "\n",
    "    def strip_leading_numbers(text):\n",
    "        return re.sub(r'^\\s*\\d+\\.\\s*', '', str(text))\n",
    "\n",
    "    # QUESTIONS ONLY\n",
    "    with open(f\"{folder}/questions.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "        for _, row in df_sorted.iterrows():\n",
    "            f.write(f\"SECTION {row['Section']}\\n\\n\")\n",
    "            questions = row[\"Questions\"]\n",
    "            options_list = row.get(\"Options\",[None]*len(questions))\n",
    "            instructions = row.get(\"Instructions\",\"\")\n",
    "            q_start = question_counter\n",
    "            q_end = question_counter + len(questions)-1\n",
    "            f.write(f\"Questions {q_start}-{q_end}\\n\\n\")\n",
    "            f.write(instructions.strip()+\"\\n\\n\")\n",
    "            for i,q_text in enumerate(questions):\n",
    "                q_text_clean = strip_leading_numbers(q_text)\n",
    "                f.write(f\"{question_counter}. {q_text_clean}\\n\")\n",
    "                opts = options_list[i] if options_list[i] else []\n",
    "                for opt in opts: f.write(f\"   {opt}\\n\")\n",
    "                question_counter+=1\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "    # FULL SET\n",
    "    question_counter=1\n",
    "    with open(f\"{folder}/full_set.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "        for _, row in df_sorted.iterrows():\n",
    "            f.write(f\"SECTION {row['Section']}\\n\\n\")\n",
    "            questions = row[\"Questions\"]\n",
    "            options_list = row.get(\"Options\",[None]*len(questions))\n",
    "            answers = row[\"Answers\"]\n",
    "            transcript = row.get(\"Transcript\",\"\")\n",
    "            instructions = row.get(\"Instructions\",\"\")\n",
    "            q_start = question_counter\n",
    "            q_end = question_counter + len(questions)-1\n",
    "            f.write(f\"Questions {q_start}-{q_end}\\n\\n\")\n",
    "            f.write(\"Instructions:\\n\"+instructions.strip()+\"\\n\\n\")\n",
    "            f.write(\"Questions:\\n\")\n",
    "            for i,q_text in enumerate(questions):\n",
    "                q_text_clean = strip_leading_numbers(q_text)\n",
    "                f.write(f\"{question_counter}. {q_text_clean}\\n\")\n",
    "                opts = options_list[i] if options_list[i] else []\n",
    "                for opt in opts: f.write(f\"   {opt}\\n\")\n",
    "                question_counter+=1\n",
    "            f.write(\"\\nAnswers:\\n\")\n",
    "            for i,a in enumerate(answers):\n",
    "                f.write(f\"{q_start+i}. {a}\\n\")\n",
    "            f.write(\"\\nTranscript:\\n\"+transcript.strip()+\"\\n\\n\\n\")\n",
    "\n",
    "    # TRANSCRIPTS ONLY\n",
    "    with open(f\"{folder}/transcripts.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "        for _, row in df_sorted.iterrows():\n",
    "            f.write(f\"SECTION {row['Section']}\\n\")\n",
    "            f.write(str(row[\"Transcript\"])+\"\\n\\n\")\n",
    "\n",
    "def save_all_outputs(questions):\n",
    "    if all(isinstance(q,pd.DataFrame) for q in questions):\n",
    "        df=pd.concat(questions,ignore_index=True)\n",
    "    else:\n",
    "        df=pd.DataFrame(questions)\n",
    "    save_csv(df)\n",
    "    folder=create_set_folder()\n",
    "    write_text_files(folder,df)\n",
    "    print(f\"All outputs saved to folder: {folder}\")\n",
    "\n",
    "# -----------------------------\n",
    "# RUN\n",
    "# -----------------------------\n",
    "theme=\"Education\"\n",
    "specific_topic=\"University Lectures\"\n",
    "specifications=\"Academic context, formal tone\"\n",
    "\n",
    "generate_question(theme,specific_topic,specifications)\n",
    "save_all_outputs(questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b952b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
