{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d95669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nTask: \\n- Make sure is 40 questions\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Task: \n",
    "- Make sure is 40 questions\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf5e2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import json5\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import textstat\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b05639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize genai\n",
    "genai.configure(api_key=\"AIzaSyBS-2pbdjYouOkcqHaX4ZI5HHPpSSmq3iw\")\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b16762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializer\n",
    "QUESTION_TYPE_CSV = \"model_training/processed_data/questionType.csv\"\n",
    "TRAINING_CSV = \"model_training/processed_data/training_set.csv\"\n",
    "GENERATED_CSV = \"model_training/processed_data/generated_question.csv\"\n",
    "WORD_CSV = \"model_training/processed_data/ielts_vocab.csv\"\n",
    "\n",
    "MAX_ATTEMPT = 5\n",
    "REWARD_GOAL = 4\n",
    "\n",
    "assignments = {}\n",
    "questions = []\n",
    "\n",
    "question_type_df = pd.read_csv(QUESTION_TYPE_CSV)\n",
    "common_vocab_df = pd.read_csv(WORD_CSV)\n",
    "training_df = pd.read_csv(TRAINING_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce80b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Question TypeError\n",
    "def choose_question_type():\n",
    "    for section in sorted(question_type_df['part'].unique()):\n",
    "        section_types = question_type_df[question_type_df['part'] == section]['type'].tolist()\n",
    "        k = random.choice([1,2])\n",
    "        selected = random.sample(section_types, k=min(k,len(section_types)))\n",
    "        assignments[f\"Section {section}\"] = selected\n",
    "    return assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73efca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT TEMPLATE\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert IELTS Listening question generator.\n",
    "Generate a realistic IELTS Listening question according to the following details:\n",
    "\n",
    "Question Type: {typeID} - {type_name}\n",
    "Theme: {theme}\n",
    "Specific Topic: {specific_topic}\n",
    "Specifications: {specifications}\n",
    "Instructions: {instruction}\n",
    "Format: {format}\n",
    "Answer Format: {answer_format}\n",
    "Key Skills: {key_skills}\n",
    "Average Duration: {avg_duration}\n",
    "Average Script Length: {avg_script_length}\n",
    "Key Features: {key_features}\n",
    "Audio Speed: {audio_speed}\n",
    "\n",
    "Requirements:\n",
    "1. Generate questions, answers, and the audio transcript.\n",
    "2. If the question type requires a diagram (Map, Plan, Flow Chart), generate a simple diagram by using text and characters.\n",
    "3. Return the output in JSON format:\n",
    "{{\n",
    "  \"Type\": [],\n",
    "  \"Instructions\": [],\n",
    "  \"Questions\": [],\n",
    "  \"Answers\": [],\n",
    "  \"Transcript\": \"\"\n",
    "}}\n",
    "Type -> Question types with type names only\n",
    "Instructions -> Instructions for the question based on the references above\n",
    "Questions -> A simple diagram for map, plan or flow chart if applicable and list of questions.\n",
    "Answers -> List of answers\n",
    "Transcript -> Full audio transcript. The transcript should include the introductions as a real IELTS Listening test. The length must between {avg_script_length}.\n",
    "4. Ensure the JSON is properly formatted. Do not include any explanations or additional text outside the JSON.\n",
    "5. Do not add other fields other than the ones mentioned in the JSON format above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b6e4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Function\n",
    "def calculate_readability_score(text):\n",
    "    return textstat.flesch_reading_ease(text)\n",
    "\n",
    "def is_in_average_word_count(text, section):\n",
    "    def calculate_total_word_count():\n",
    "        nonlocal text\n",
    "        if isinstance(text, (list, pd.Series)):\n",
    "            text = \" \".join(map(str, text))\n",
    "\n",
    "        words = re.findall(r'\\b\\w+\\b', str(text))\n",
    "        return len(words)\n",
    "    \n",
    "    expected_ranges = {\n",
    "        \"Section 1\": (500, 700),\n",
    "        \"Section 2\": (600, 800),\n",
    "        \"Section 3\": (800, 1000),\n",
    "        \"Section 4\": (1000, 1200)\n",
    "    }\n",
    "\n",
    "    total_words = calculate_total_word_count()\n",
    "\n",
    "    low, high = expected_ranges[section]\n",
    "    return low <= total_words <= high\n",
    "\n",
    "def calculate_common_word_ratio(text):\n",
    "    common_vocab = set(common_vocab_df[\"Words\"].astype(str).str.lower().tolist())\n",
    "    \n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        return 0\n",
    "    \n",
    "    words = [w.lower() for w in re.findall(r'\\b\\w+\\b', str(text))]\n",
    "    if not words:\n",
    "        return 0\n",
    "    \n",
    "    uncommon = [w for w in words if w not in common_vocab]\n",
    "    return len(uncommon) / len(words)\n",
    "\n",
    "def calculate_similarity(text):\n",
    "    existing_texts = []\n",
    "\n",
    "    if 'transcript' in training_df.columns:\n",
    "        existing_texts += training_df['transcript'].dropna().astype(str).tolist()\n",
    "\n",
    "    if os.path.exists(GENERATED_CSV):\n",
    "        generated_df = pd.read_csv(GENERATED_CSV)\n",
    "        if 'Transcript' in generated_df.columns:\n",
    "            existing_texts += generated_df['Transcript'].dropna().astype(str).tolist()\n",
    "\n",
    "        if not existing_texts:\n",
    "            return 0.0\n",
    "    \n",
    "    corpus = existing_texts + [str(text)]\n",
    "\n",
    "    vectorizer = TfidfVectorizer().fit(corpus)\n",
    "    vectors = vectorizer.transform(corpus)\n",
    "\n",
    "    sim_scores = cosine_similarity(vectors[-1], vectors[:-1]).flatten()\n",
    "\n",
    "    max_sim = sim_scores.max() if len(sim_scores) > 0 else 0.0\n",
    "    return max_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d860b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Function\n",
    "def safe_json_parse(raw_text):\n",
    "    start = raw_text.find(\"{\")\n",
    "    end = raw_text.rfind(\"}\")\n",
    "    \n",
    "    if start == -1 or end == -1:\n",
    "        return None\n",
    "\n",
    "    json_block = raw_text[start:end+1]\n",
    "    cleaned = json_block\n",
    "    cleaned = cleaned.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    cleaned = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", cleaned)\n",
    "    cleaned = re.sub(r\",\\s*([\\]}])\", r\"\\1\", cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "\n",
    "    try:\n",
    "        parsed = json5.loads(cleaned)\n",
    "        return parsed\n",
    "\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2430ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "def convert_to_df(questions, section):\n",
    "    def normalize_question(q):\n",
    "        if q is None:\n",
    "            return None\n",
    "        \n",
    "        clean = {}\n",
    "\n",
    "        clean['DateTime_Generated'] = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "\n",
    "        if section == \"\":\n",
    "            clean['Section'] = section\n",
    "        else:\n",
    "            clean['Section'] = q.get('Section')\n",
    "            \n",
    "        # Flatten Type\n",
    "        t = q.get(\"Type\")\n",
    "        if isinstance(t, list):\n",
    "            clean[\"Type\"] = \" \".join(str(x) for x in t)\n",
    "        else:\n",
    "            clean[\"Type\"] = t\n",
    "\n",
    "        # Flatten Instructions\n",
    "        instr = q.get(\"Instructions\")\n",
    "        if isinstance(instr, list):\n",
    "            clean[\"Instructions\"] = \" \".join(str(x) for x in instr)\n",
    "        else:\n",
    "            clean[\"Instructions\"] = instr\n",
    "\n",
    "        clean[\"Questions\"] = q.get(\"Questions\")\n",
    "        clean[\"Answers\"] = q.get(\"Answers\")\n",
    "        clean[\"Transcript\"] = q.get(\"Transcript\")\n",
    "\n",
    "        return clean\n",
    "    \n",
    "    return pd.DataFrame([normalize_question(q) for q in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5114d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Question\n",
    "def generate_question_model(typeID, type_name, theme, specific_topic, specifications, instruction, answer_format, format, key_skills, avg_duration, avg_script_length, key_features, audio_speed):\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        typeID=typeID,\n",
    "        type_name=type_name,\n",
    "        theme=theme,\n",
    "        specific_topic=specific_topic,\n",
    "        specifications=specifications,\n",
    "        instruction=instruction,\n",
    "        answer_format=answer_format,\n",
    "        format=format,\n",
    "        key_skills=key_skills,\n",
    "        avg_duration=avg_duration,\n",
    "        avg_script_length=avg_script_length,\n",
    "        key_features=key_features,\n",
    "        audio_speed=audio_speed\n",
    "    )\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    clean_response = safe_json_parse(response.text)\n",
    "    \n",
    "    return clean_response\n",
    "\n",
    "def generate_question(theme, specific_topic, specifications):\n",
    "    assignments = choose_question_type()\n",
    "    for section, types in assignments.items():\n",
    "        print(f\"\\n{section} Questions:\")\n",
    "        for q_type in types:\n",
    "            type_info = question_type_df[question_type_df['type'] == q_type].iloc[0]\n",
    "\n",
    "            for x in range(MAX_ATTEMPT):\n",
    "                print(f\"  Try Attempt: {x}\")\n",
    "                question_data = generate_question_model(\n",
    "                    typeID=type_info['type'],\n",
    "                    type_name=type_info['type'],\n",
    "                    theme=theme,\n",
    "                    specific_topic=specific_topic,\n",
    "                    specifications=specifications,\n",
    "                    instruction=type_info['instruction'],\n",
    "                    answer_format=type_info['answer_format'],\n",
    "                    format=type_info['format'],\n",
    "                    key_skills=type_info['key_skills'],\n",
    "                    avg_duration=type_info['avg_duration'],\n",
    "                    avg_script_length=type_info['avg_script_length'],\n",
    "                    key_features=type_info['key_features'],\n",
    "                    audio_speed=type_info['audio_speed']\n",
    "                )\n",
    "                \n",
    "                temp_df = convert_to_df([question_data], section)\n",
    "                if 'Transcript' in temp_df.columns and pd.notna(temp_df['Transcript'].iloc[0]):\n",
    "                    transcript_text = temp_df['Transcript'].iloc[0]\n",
    "                    reability_score = calculate_readability_score(transcript_text)\n",
    "                    avg_word_length = is_in_average_word_count(transcript_text, section)\n",
    "                    common_word_ratio = calculate_common_word_ratio(transcript_text)\n",
    "                    similarity = calculate_similarity(transcript_text)\n",
    "                    reward = 0\n",
    "                else:\n",
    "                    transcript_text = \"\"\n",
    "                    reability_score = 0\n",
    "                    avg_word_length = False\n",
    "                    common_word_ratio = 0\n",
    "                    similarity = 0\n",
    "                    reward = 0\n",
    "\n",
    "                print(f\"    Current Score: {reability_score} | {avg_word_length} | {common_word_ratio} | {similarity}\")\n",
    "\n",
    "                # Calculate Reward\n",
    "                if reability_score >= 60:\n",
    "                    reward += 1\n",
    "                if avg_word_length:\n",
    "                    reward += 1\n",
    "                if common_word_ratio >= 0.1:\n",
    "                    reward += 1\n",
    "                if similarity <= 0.85:\n",
    "                    reward += 1\n",
    "\n",
    "                print(f\"    Current Reward: {reward}\")\n",
    "\n",
    "                if reward == REWARD_GOAL:\n",
    "                    break\n",
    "\n",
    "            questions.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fffaa97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Section 1 Questions:\n",
      "  Try Attempt: 0\n",
      "    Current Score: 72.87268558468857 | True | 0.2833935018050541 | 0.467939455502918\n",
      "    Current Reward: 4\n",
      "\n",
      "Section 2 Questions:\n",
      "  Try Attempt: 0\n",
      "    Current Score: 61.288388463641354 | False | 0.18421052631578946 | 0.6050274123924136\n",
      "    Current Reward: 3\n",
      "  Try Attempt: 1\n",
      "    Current Score: 57.00673174872668 | False | 0.18115942028985507 | 0.6163843612919224\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 2\n",
      "    Current Score: 56.19629900213829 | False | 0.1753958587088916 | 0.6080322619666219\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 3\n",
      "    Current Score: 43.41327615780449 | False | 0.18885096700796358 | 0.6317757514840734\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 4\n",
      "    Current Score: 0 | False | 0 | 0\n",
      "    Current Reward: 1\n",
      "\n",
      "Section 3 Questions:\n",
      "  Try Attempt: 0\n",
      "    Current Score: 0 | False | 0 | 0\n",
      "    Current Reward: 1\n",
      "  Try Attempt: 1\n",
      "    Current Score: 14.846695979899494 | False | 0.25895765472312704 | 0.476714777574597\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 2\n",
      "    Current Score: 30.009974606558444 | False | 0.17772215269086358 | 0.6864706469779782\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 3\n",
      "    Current Score: 21.468486326563266 | False | 0.19424460431654678 | 0.5559195669020365\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 4\n",
      "    Current Score: 24.93786375105131 | False | 0.19863945578231293 | 0.4933187989484724\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 0\n",
      "    Current Score: 53.49892623873876 | False | 0.2073288331726133 | 0.4755617600576927\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 1\n",
      "    Current Score: 49.04642445965027 | False | 0.2389463781749765 | 0.5733240860792368\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 2\n",
      "    Current Score: 50.5434458348557 | False | 0.18834080717488788 | 0.3750057645810595\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 3\n",
      "    Current Score: 52.841959549071646 | False | 0.18096054888507718 | 0.580994144362539\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 4\n",
      "    Current Score: 54.895962037047894 | False | 0.1925581395348837 | 0.42184167427705077\n",
      "    Current Reward: 2\n",
      "\n",
      "Section 4 Questions:\n",
      "  Try Attempt: 0\n",
      "    Current Score: 12.694583333333355 | False | 0.19903498190591074 | 0.7092802824199678\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 1\n",
      "    Current Score: 22.353729653220114 | False | 0.21003134796238246 | 0.6888516141241589\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 2\n",
      "    Current Score: 0 | False | 0 | 0\n",
      "    Current Reward: 1\n",
      "  Try Attempt: 3\n",
      "    Current Score: 12.92271959459464 | True | 0.21994134897360704 | 0.7047943665675813\n",
      "    Current Reward: 3\n",
      "  Try Attempt: 4\n",
      "    Current Score: 8.000996638655465 | False | 0.2236976506639428 | 0.7325293147045907\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 0\n",
      "    Current Score: -2.504978905072818 | True | 0.2678088367899008 | 0.468645034326679\n",
      "    Current Reward: 3\n",
      "  Try Attempt: 1\n",
      "    Current Score: 18.487727272727284 | False | 0.22661122661122662 | 0.603301736044786\n",
      "    Current Reward: 2\n",
      "  Try Attempt: 2\n",
      "    Current Score: 17.832762803234516 | True | 0.21256931608133087 | 0.7348985943279991\n",
      "    Current Reward: 3\n",
      "  Try Attempt: 3\n",
      "    Current Score: -2.2643325086590664 | True | 0.2604355716878403 | 0.6802550580803504\n",
      "    Current Reward: 3\n",
      "  Try Attempt: 4\n",
      "    Current Score: 11.918373717853456 | False | 0.2128966223132037 | 0.6884969527471416\n",
      "    Current Reward: 2\n"
     ]
    }
   ],
   "source": [
    "# Main Pipeline\n",
    "theme = \"Education\"\n",
    "specific_topic = \"University Lectures\"\n",
    "specifications = \"Academic context, formal tone\"\n",
    "\n",
    "generate_question(theme, specific_topic, specifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f768d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 76\u001b[0m\n\u001b[0;32m     72\u001b[0m     write_text_files(folder_path, df)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll outputs saved to folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m save_all_outputs(\u001b[43mquestions\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'questions' is not defined"
     ]
    }
   ],
   "source": [
    "# Save to Flie\n",
    "def save_csv(df):\n",
    "    os.makedirs(\"model_training/processed_data\", exist_ok=True)\n",
    "    if os.path.exists(GENERATED_CSV):\n",
    "        df.to_csv(GENERATED_CSV, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(GENERATED_CSV, index=False)\n",
    "    print(f\"Saved generated questions to {GENERATED_CSV}\")\n",
    "\n",
    "def create_set_folder():\n",
    "    os.makedirs(\"set\", exist_ok=True)\n",
    "    existing = [d for d in os.listdir(\"set\") if d.startswith(\"set\")]\n",
    "    next_id = len(existing) + 1\n",
    "    folder_path = f\"set/set{next_id}\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    return folder_path\n",
    "\n",
    "def write_text_files(folder, df):\n",
    "    sections = {\n",
    "        1: df.iloc[0],\n",
    "        2: df.iloc[1],\n",
    "        3: df.iloc[2],\n",
    "        4: df.iloc[3]\n",
    "    }\n",
    "\n",
    "    # Questions Only\n",
    "    with open(f\"{folder}/questions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for sec, row in sections.items():\n",
    "            f.write(f\"Section {sec}\\n\")\n",
    "            f.write(\"Instructions:\\n\")\n",
    "            f.write(str(row[\"Instructions\"]) + \"\\n\\n\")\n",
    "            for q in row[\"Questions\"]:\n",
    "                f.write(str(q) + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Full set: Instructions + Questions + Answers + Transcript\n",
    "    with open(f\"{folder}/full_set.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for sec, row in sections.items():\n",
    "            f.write(f\"Section {sec}\\n\")\n",
    "\n",
    "            f.write(\"Instructions:\\n\")\n",
    "            f.write(str(row[\"Instructions\"]) + \"\\n\\n\")\n",
    "\n",
    "            f.write(\"Questions:\\n\")\n",
    "            for q in row[\"Questions\"]:\n",
    "                f.write(str(q) + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"Answers:\\n\")\n",
    "            for a in row[\"Answers\"]:\n",
    "                f.write(str(a) + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            f.write(\"Transcript:\\n\")\n",
    "            f.write(str(row[\"Transcript\"]) + \"\\n\")\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "    # Transcripts Only\n",
    "    with open(f\"{folder}/transcripts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for sec, row in sections.items():\n",
    "            f.write(f\"Section {sec}\\n\")\n",
    "            f.write(str(row[\"Transcript\"]) + \"\\n\\n\")\n",
    "\n",
    "def save_all_outputs(questions):\n",
    "    if all(isinstance(q, pd.DataFrame) for q in questions):\n",
    "        df = pd.concat(questions, ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame(questions)\n",
    "    \n",
    "    save_csv(df)\n",
    "    folder_path = create_set_folder()\n",
    "    write_text_files(folder_path, df)\n",
    "    print(f\"All outputs saved to folder: {folder_path}\")\n",
    "\n",
    "\n",
    "save_all_outputs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
